//***********************************************
// Dynamic Programming notes
//***********************************************

- Dynamic programming is similar to divide and conquer in that problems are made into smaller problems, and then combining the solutions of those smaller problems.
    - When dividing and conquering problems, however, there may be overlap between subproblems, in other words, subproblems share subproblems.
    - A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblem.

- We typically apply dynamic programming to optimization problems, like getting a max profit/value or minimizing cost.

- When developing a dynamic-programming algorithm, we follow a sequence of four steps:
    1. Characterize the structure of an optimal solution.
    2. Recursively define the value of an optimal solution.
    3. Compute the value of an optimal solution, typically in a bottom-up fashion.
        - at this point we have the value of an optimal solution, not the solution itself (that is what step 4 is for)
    4. Construct an optimal solution from computed information.

    Obviously, do not start coding right away; ponder the problem, look at patterns, work out some math, then redo it in a more organized way, then work out pseudo code, trace that pseudo code, and then write code once you know (more or less at least) will work.


//===============================================
 Elements of dynamic programming
//===============================================
There are two essental "ingredients" that an optimization problem must have in order for dynamic programming to apply:
    - optimal subtructure
    - overlapping subproblems

 Optimal Substructure
------------------------
First step is to characterize the structure of an optimal solution.
* A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems.

In dynamic programming, we build an optimal solution to the problem from optimal solutions to subproblems.

- In the rod cutting problem, it was observed that the optimal way to of cutting up a rod of length n (if any cuts were made) involves optimally cutting up the pieces resulting from the first cut.

- For the matrix chain parenthesization problem, it was observed that the optimal parenthesization of a chain of matrices from i, i+1, ..., j that splits the product between matrix k and k+1 contains within it optimal solutions to the problems of parenthesizing matrices i, i+1, ..., k, k+1, ..., j.

Optimal subtructure varies across problem domains in two ways:
    1. How many subproblems an optimal solution to the original problem uses, and
    2. how many choices we have in determining which subproblem(s) to use in an optimal solution.
    
    In the rod cutting problem, an optimal solution for cutting up a rod of size n uses just one subproblem (of size n-i), but we must consider n choices for i in order to determine which one yields an optimal solution.
    
    Matrix-chain multiplication for a subchain serves as an example with two subproblems and j-i choices. 
    For a given matrix at which we split the product, we have two subproblems--parenthesizing matrices i, i+1, ...k and parenthesizing matrices k+1, k+2, ..., j--and we must solve both of them optimally.
    Once we determine the optimal solutions to subproblems, we choose among j-i candidates for the index k.
    
Informally, the running time of a dynamic-programming algorithm depends on the product of two factors:
    - number of subproblems, and
    - how many choices we look at for each subproblem.
    
    In rod cutting, we had Theta(n) subproblems overall, and at most n choices to examine for each, yielding O(n^2) time.
    Matrix-chain multiplication had Theta(n^2) subproblems overall, and in each we had at most n-1 choices, giving an O(n^3) running time (actually Theta(n^3)).

*Independence: the solution to one subproblem does not affect the solution to another subproblem of the same problem.
    - In rod cutting, to determine the best way to cut up a rod of length n, we look at the best ways of cutting up rods of length i = 0, 1, ..., n-1. Because an optimal solution to the length-n problem includes just one of these subproblem solutions (after cutting off the first piece), independence of subproblems is not an issue.
    - In matrix-chain multiplication, the subproblems are multiplying subchains i, i+1...k and k+1, k+2, ..., j. These subchains are disjoint, so that no matrix could possibly be included in both of them.

 Overlapping Subproblems
------------------------
When a recursive algorithm revisits the same problem repeatedly, the optimization problem has overlapping subproblems.
    - In contrast, divide and conquer generates new subproblems at each recursion.
    Dynamic programming takes advantage of overlapping subproblems by storing new solutions. 
