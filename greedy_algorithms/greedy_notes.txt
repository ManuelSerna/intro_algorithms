//***********************************************
 Greedy algorithms notes.
//***********************************************

For many optimization problems, suing dynamic programming to determine the best choices is overkill; simpler, more efficient algorithms will do.
*A greedy algorithm always makes the choice that looks best at the moment. That is, it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution.

Greedy algorithms don't always give the best value, but it does for many problems.

//-----------------------------------------------
 Elements of the greedy strategy.
//-----------------------------------------------

- Referencing the activity selection problem, we went through the following steps that was a bit more involved that usual:
    1. Determine the optimal substructure of the problem.
    
    2. Develop a recursive solution (but in the case of activity selection, we can develop an iterative solution)
    
    3. Show that if we make the greedy choice, then only one subproblem remains.
    
    4. Prove that it is always safe to make the greedy choice (#3 and #4 can be swapped).
    
    5. Develop a recursive algorithm that implements the greedy strategy.
    
    6. Convert the recursive algorithm to an iterative algorithm.
    

- More generally, we design greedy algorithms according to the following sequence of steps:
    1. Cast the optimization problem as one in which we make a choice and are left with one subproblem to solve.
    
    2. Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.
    
    3. Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if we combine an optimal solution to the subproblem with the greedy choice we have made, we arrive at an optimal solution to the original problem.
    
Unfortunately there is no one way of telling if a greedy approach can solve all optimization problems. There are, however, two key ingredients.
    
    1. Greedy choice property.
        * We can assemble a globally optimal solution by making locally optimal (greedy) choices, we don't consider results from subproblems.
        
            - Here is where greedy and dynamic programming approaches differ--dynamic programming makes choices at each step, depending on the solutions to subproblems. 
            Greedy algorithms run with the best choice in the moment and the solve the subproblem that remains. The choice made by a greedy algorithm may depend of choices so far, but it cannot depend on any future choices or on the solutions to subproblems.
            
        We can usually make the greedy choice more efficiently than when we have to consider a wider set of choices. By preprocessing the input or by using an appropriate data structure (often a priority queue), we often can make greedy choices quickly, thus yielding an efficient algorithm.
    
    2. Optimal substructure
        *
